\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{latexsym}
\usepackage[english]{babel}
\usepackage[a4paper]{geometry}
\usepackage[sumlimits]{amsmath}
\usepackage{amssymb}
\usepackage[nomessages]{fp}
\usepackage{pst-all}
\usepackage[labelformat=simple]{caption}
\usepackage{multido}
\usepackage[normalem]{ulem}

\parindent0pt

\title{Math}
\author{Friedemann Zintel}
\date{\today}

\newcommand{\spc}{\hspace{0.2cm}}
\newcommand*{\vecdd}[2]{\begin{pmatrix}#1\\#2\\\end{pmatrix}}
\newcommand{\follows}{\Rightarrow}
\newcommand{\contra}{\Rightarrow\Leftarrow}

\begin{document}
\tableofcontents
\newpage
\part{Proofs}
\section{Orthogonality and determinant}
Be $\vec{v_1}=\vecdd{x_1}{y_1}=|\vec{v_1}|\vecdd{\cos\alpha}{\sin\alpha}$, $\vec{v_2}=\vecdd{x_2}{y_2}=|\vec{v_2}|\vecdd{\cos\beta}{\sin\beta}$
with $\alpha=\measuredangle\vec{v_1}$, $\beta=\measuredangle\vec{v_2}$
\\\\\\\\
\subsection*{Orthogonality}
To proove: $\vec{v_1}\perp \vec{v_2} <=> \vec{v_1}*\vec{v_2}=0$ :\\\\
$\vec{v_1}*\vec{v_2}
=|\vec{v_1}|\vecdd{\cos\alpha}{\sin\alpha}|\vec{v_2}|\vecdd{\cos\beta}{\sin\beta}
=|\vec{v_1}||\vec{v_2}|(\cos\alpha\cos\beta+\sin\alpha\sin\beta)=\uuline{|\vec{v_1}||\vec{v_2}|\cos(\alpha-\beta)}$.
\\\\\\\\
Be $\vec{v_1}\perp \vec{v_2} <=> \alpha=\beta+\frac{\pi}{2}$, w.l.o.g.
\\\\
$=>|\vec{v_1}||\vec{v_2}|\cos(\alpha-\beta)=|\vec{v_1}||\vec{v_2}|\cos(\beta+\frac{\pi}{2}-\beta)=0$
\\\\$\square$
\subsection*{Determinant}
Let's choose $\sin()$ instead of $\cos()$ in the above equation:
\\\\
$|\vec{v_1}||\vec{v_2}|\sin(\alpha-\beta)=|\vec{v_1}||\vec{v_2}|(\sin\alpha\cos\beta-\cos\alpha\sin\beta)$
\\\\
$=|\vec{v_1}||\vec{v_2}|\vecdd{\sin\alpha}{\cos\alpha}\vecdd{\cos\beta}{-\sin\beta}$
$=\vecdd{|\vec{v_1}|\sin\alpha}{|\vec{v_1}|\cos\alpha}\vecdd{|\vec{v_2}|\cos\beta}{-|\vec{v_2}|\sin\beta}=\vecdd{y_1}{x_1}\vecdd{x_2}{-y_2}$
\\\\
$=\det\left(\begin{array}{ll}x_2&x_1\\y_2&y_1\\\end{array}\right)=\uuline{\det\left(\begin{array}{ll}\vec{v_2}&\vec{v_1}\\\end{array}\right)}.$
\\\\\\
If $\alpha=\beta+k*\pi$, w.l.o.g, $k\in\mathbb{N}$
\\\\
$=> \det\left(\begin{array}{ll}\vec{v_2}&\vec{v_1}\\\end{array}\right)=|\vec{v_1}||\vec{v_2}|\sin(\alpha-\beta)=|\vec{v_1}||\vec{v_2}|\sin(\beta+k*\pi-\beta)=0$
\\\\
$=> \vec{v_1}$ and $\vec{v_2}$ are linear dependent and the determinant in general determines linear dependency.
\newpage
\section{Law of cosine}
To proove: $c^2 = a^2+b^2-2ab\cos\gamma$
\\\\\\
$\gamma>=\frac{\pi}{2}:$
\begin{align}
d^2=b^2-e^2\\
e=b\sin(\pi-\gamma)\\
1=\sin^2\gamma+\cos^2\gamma
\end{align}
\\\\
\begin{align*}
c^2 = (a+d)^2+e^2
\\\\
=(a+\sqrt{b^2-e^2})^2+e^2
\\\\
=a^2+2a\sqrt{b^2-e^2}+b^2
\\\\
=a^2+2a\sqrt{b^2-b^2\sin^2(\pi-\gamma)}+b^2
\\\\
=a^2+2ab\sqrt{1-\sin^2(\pi-\gamma)}+b^2
\\\\
=a^2+2ab\sqrt{\cos^2(\pi-\gamma)}+b^2
\\\\
=a^2+b^2+2ab\cos(\pi-\gamma)
\\\\
=a^2+b^2-2ab\cos\gamma
\end{align*}
\\\\
$\square$
\newpage
\section{Polynomial derivation}
To proove: $(x^n)' = nx^{n-1}$
\\\\\\
\begin{align*}
\frac{f(x+\vartriangle x)-f(x)}{\vartriangle x}
\\\\
=\frac{(x+\vartriangle x)^n-x^n}{\vartriangle x}
\\\\
=\frac{\sum_{k=0}^{n}{n \choose k }(x^{n-k}\vartriangle x^k)-x^n}{\vartriangle x}
\\\\
=\frac{x^n\Delta x^0+\sum_{k=1}^{n}{n \choose k}(x^{n-k}\Delta x^k)-x^n}{\Delta x}
\\\\
=\frac{\sum_{k=1}^{n}{n \choose k}(x^{n-k}\Delta x^k)}{\Delta x}
\\\\
=\frac{\Delta x\sum_{k-1}^{n}{n \choose k}(x^{n-k}\Delta x^{k-1})}{\Delta x}
\\
=\sum_{k-1}^{n}{n \choose k}(x^{n-k}\Delta x^{k-1})
\\\\
=\sum_{k=1}^{n}\frac{n!}{k!(n-k)!}(x^{n-k}\Delta x^{k-1})
\\\\
=nx^{n-1}+\sum_{k=2}^{n}{n \choose k}(x^{n-k}\Delta x^k)
\\\\
\lim_{\Delta x \to 0}\left(nx^{n-1}+\sum_{k=2}^{n}{n \choose k}(x^{n-k}\Delta x^k)\right) = nx^{n-1}
\end{align*}
\\\\
$\square$
\newpage
\section{Binomial theorem}
To proove: $(x+y)^n=\sum_{k=0}^{n}{n \choose k}(x^{n-k}y^k)$
\\\\
Induction:\\
$n=0$:
\begin{align*}
(x+y)^0=1=\sum_{k=0}^{0}{0 \choose k}(x^{0-k}y^k)
\end{align*}
$n \to n+1$:
\begin{align*}
(x+y)^{n+1}=(x+y)^n(x+y)\stackrel{\textnormal{i.p.}}{=}\sum_{k=0}^{n}{n \choose k}(x^{n-k}y^{k})(x+y)
\\\\
=\sum_{k=0}^{n}{n \choose k}(x^{n+1-k}y^k)+\sum_{k=0}^{n}{n \choose k}(x^{n-k}y^{k+1})
\\\\
=\sum_{k=0}^{n}{n \choose k}(x^{n+1-k}y^k)+\sum_{k=1}^{n+1}{n \choose k-1}(x^{n+1-k}y^{k})
\\\\
={n \choose 0}x^{n+1}y^0+\sum_{k=1}^{n}{n \choose k}(x^{n+1-k}y^k)+{n \choose n}x^0y^{n+1}+\sum_{k=1}^{n}{n \choose k-1}(x^{n+1-k}y^k)
\\\\
={n+1 \choose 0}x^{n+1}y^0+\sum_{k=1}^{n}{n \choose k}(x^{n+1-k}y^k)+{n+1 \choose n+1}x^0y^{n+1}+\sum_{k=1}^{n}{n \choose k-1}(x^{n+1-k}y^k)
\\\\
={n+1 \choose 0}x^{n+1}y^0+\sum_{k=1}^{n}{n \choose k-1}(x^{n+1-k}y^k)+\sum_{k=1}^{n}{n \choose k}(x^{n+1-k}y^k)+{n+1 \choose n+1}x^0y^{n+1}
\\\\
=^*{n+1 \choose 0}x^{n+1}y^0+\sum_{k=1}^{n}\left({n \choose k-1}+{n \choose k}\right)(x^{n+1-k}y^k)+{n+1 \choose n+1}x^0y^{n+1}
\\\\
={n+1 \choose 0}x^{n+1}y^0+\sum_{k=1}^{n}{n+1 \choose k}(x^{n+1-k}y^k)+{n+1 \choose n+1}x^0y^{n+1}
\\\\
=\sum_{k=0}^{n+1}{n+1 \choose k}(x^{n+1-k}y^k)
\end{align*}
\\\\
$\square$
\newpage
\begin{align*}
^*
\\
{n \choose k-1}+{n \choose k}=\frac{n!}{(k-1)!(n-k+1)!}+\frac{n!}{k!(n-k)!}
\\\\
=\frac{n!k}{k!(n-k+1)!}+\frac{n!(n-k+1)}{k!(n-k+1)!}
\\\\
=\frac{n!k+n!(n-k+1)}{k!(n+1-k)!}
\\\\
=\frac{n!(k+n+1-k)}{k!(n+1-k)!}
\\\\
=\frac{(n+1)!}{k!(n+1-k)!}
\\\\
=\uuline{{n+1 \choose k}}
\end{align*}
\newpage
\section{Linear regression}
measured values $x_i,y_i$ $|$ $1<=i<=n, n\in \mathbb{N}$
\\\\
regression line: $y=mx+b$\\
minimize error by calculating least squares
\\\\
\begin{align*}
S=\sum_{i=1}^{n}(y_i-(mx_i+b))^2=\sum_{i=1}^{n}(y_i-mx_i-b)^2
\end{align*}
\\\\
set $\frac{\partial S}{\partial m}=0$ :
\begin{align*}
\frac{\partial S}{\partial m}=-2\sum_{i=1}^{n}(y_i-mx_i-b)x_i=0
\\\\
\Longleftrightarrow \quad 0=\sum_{i=1}^{n}(y_i-mx_i-b)x_i=\sum_{i=1}^{n}(x_iy_i-mx_ix_i-bx_i)
\\\\
=\sum_{i=1}^{n}x_iy_i-m\sum_{i=1}^{n}x_ix_i-b\sum_{i=1}^{n}x_i \quad \textbf{: \rm I}
\end{align*}
\\\\
set $\frac{\partial S}{\partial b}=0$ :
\begin{align*}
\frac{\partial S}{\partial b}=-2\sum_{i=1}^{n}(y_i-mx_i-b)=0
\\\\
\Longleftrightarrow \quad 0=\sum_{i=1}^{n}(y_i-mx_i-b)=\sum_{i=1}^{n}(y_i-mx_i-b)=\sum_{i=1}^{n}y_i-m\sum_{i=1}^{n}x_i-nb
\\\\
\Longleftrightarrow \quad b=\frac{1}{n}\sum_{i=1}^{n}y_i-m\frac{1}{n}\sum_{i=1}^{n}x_i=\uuline{y_M-mx_M} \quad \textbf{: \rm II}
\end{align*}
\rm{II} in \rm{I}:
\begin{align*}
0=\sum_{i=1}^{n}x_iy_i-m\sum_{i=1}^{n}x_ix_i-b\sum_{i=1}^{n}x_i=\sum_{i=1}^{n}x_iy_i-m\sum_{i=1}^{n}x_ix_i-(y_M-mx_M)\sum_{i=1}^{n}x_i
\\\\
=\sum_{i=1}^{n}x_iy_i-m\sum_{i=1}^{n}x_ix_i-y_M\sum_{i=1}^{n}x_i+mx_M\sum_{i=1}^{n}x_i
\\\\
\Longleftrightarrow \quad m\left(\sum_{i=1}^{n}x_ix_i-x_M\sum_{i=1}^{n}x_i\right)=\sum_{i=1}^{n}x_iy_i-y_M\sum_{i=1}^{n}x_i
\\\\
\Longleftrightarrow \quad m=\frac{\sum_{i=1}^{n}x_iy_i-y_M\sum_{i=1}^{n}x_i}{\sum_{i=1}^{n}x_ix_i-x_M\sum_{i=1}^{n}x_i}
\\\\
=\frac{\frac{1}{n}\sum_{i=1}^{n}x_iy_i-x_My_M}{\frac{1}{n}\sum_{i=1}^{n}x_ix_i-x_Mx_M}
\\\\
=^*\uuline{\frac{Cov(x,y)}{Var(x)}}
\\\\\\\\
\Longrightarrow y=mx+n
\\\\
=\frac{Cov(x,y)}{Var(x)}x+(y_M-\frac{Cov(x,y)}{Var(x)}x_M)
\\\\
=\uuline{\frac{Cov(x,y)}{Var(x)}(x-x_M)+y_M}
\end{align*}
\newpage
\begin{align*}
^*
\\\\
Cov(x,y):=\frac{1}{n}\sum_{i=1}^{n}(x_i-x_M)(y_i-y_M)
\\
Var(x):=\frac{1}{n}\sum_{i=1}^{n}(x_i-x_M)^2=Cov(x,x)
\\\\
Cov(x,y):=\frac{1}{n}\sum_{i=1}^{n}(x_i-x_M)(y_i-y_M)
\\\\
=\frac{1}{n}\sum_{i=1}^{n}(x_iy_i-x_iy_M-x_My_i+x_My_M)
\\\\
=\frac{1}{n}\left(\sum_{i=1}^{n}x_iy_i-y_M\sum_{i=1}^{n}x_i-x_M\sum_{i=1}^{n}y_i+nx_My_M\right)
\\\\
=\frac{1}{n}\sum_{i=1}^{n}x_iy_i-y_Mx_M-x_My_M+x_My_M
\\\\
=\uuline{\frac{1}{n}\sum_{i=1}^{n}x_iy_i-x_My_M}
\end{align*}
\newpage
\section{Number theory - irrational roots}
To proove: if a root of a natural number is not integer then it is irrational\\
formal: $\forall x\in\mathbb{N}: (\sqrt{x}\not\in\mathbb{Z}) \follows (\not\exists a,b\in\mathbb{Z}: \frac{a}{b}=\sqrt{x})$
\\\\
assume $\exists a,b\in\mathbb{Z}:\frac{a}{b}=\sqrt{x},$ $a,b$ have no common divisors\\
\begin{align*}
  \follows \frac{a^2}{b^2}=x\\
  \iff a^2=x*b^2=(x*b)*b\\\\
  \text{be }\\
  A=\{p_i|p_i \text{ is i-th of n primefactors of }a\}\\
  X=\{p_j|p_j \text{ is j-th of m primefactors of }x\}\\
  B=\{p_k|p_k \text{ is k-th of l primefactors of }b\}\\\\
  \follows a=\prod_{p_i\in A}{p_i},\spc a^2=\prod_{p_i\in A}{p_i}^2,\spc x=\prod_{p_j\in X}{p_j},\spc b=\prod_{p_k\in B}{p_k}\\
  \follows a^2=\prod_{p_j\in X}{p_j}*\prod_{p_k\in B}{p_k}^2=\prod_{p_i\in A}{p_i}^2\\\\
  \text{since integer factorization is unique }\follows B\subseteq A\\ 
\end{align*}\\
\underline{case I:} $B=\emptyset$\\
$\follows b=1\follows \sqrt{x}=a$ is integer
\\\\
\underline{case II:} $B\not=\emptyset$\\
$\follows B$ is a set of common divisors of $a$ and $b\spc\contra$ to assumption
\\\\
$\square$
\newpage
\part{Computation}
\section{linear interpolation of discrete vector field}
Be $\vec{g}:\mathbb{Z}^n\rightarrow\mathbb{R}^m$\\\\
We define $\vec{f}:\mathbb{R}^n\rightarrow\mathbb{R}^m$ as:\\\\
\begin{align*}
  \vec{f}(\vec{v}):=\sum_{p\in\mathcal{P}(S)}^{}\prod_{i=1}^{n}\left((1-\vec{\delta}(\vec{v})_i)^{1-\xi_p(i)}\vec{\delta}(\vec{v})_i^{\xi_p(i)}\right)\vec{g}(\vec{h}_p(\vec{v}))
\end{align*}
with
\begin{align*}
  S:=\{x\in\mathbb{N} \mid 1\leq x\leq n\},\quad p\in\mathcal{P}(S)\\\\
  \text{indicator function:}\\
  \xi_A:\mathbb{N}\rightarrow\{0,1\},\quad A \subseteq \mathbb{N}\\
  \xi_A(x):=
  \begin{cases}
    1&x\in A\\
    0&x\not\in A
  \end{cases}\\\\
  \text{discretization function:}\\
  \vec{h}_p:\mathbb{R}^n\rightarrow\mathbb{Z}^n\\
  \quad h_i:=
  \begin{cases}
    \lceil v_i\rceil&i\in p\\
    \lfloor v_i\rfloor&i\not\in p
  \end{cases}\\
  \text{with}\quad \vec{h}:=\vec{h}_p(\vec{v})
\\\\
  \text{delta function:}\\
  \vec{\delta}:\mathbb{R}^n\rightarrow \{z\in\mathbb{R}\mid 0\leq z<1\}^n\\
  \vec{\delta}(\vec{v}):=\vec{v}-\vec{h}_{\emptyset}(\vec{v})
\end{align*}
.
\\\\
\uline{Example}: $n=2,\quad d_1:=\vec{\delta}(\vec{v})_1,d_2:=\vec{\delta}(\vec{v})_2$\\
\begin{align*}
  \vec{f}(\vec{v})=\vec{f}(\vec{h_{\emptyset}}(\vec{v})+\vec{\delta}(\vec{v}))=\vec{f}(\vecdd{\lfloor v_1\rfloor}{\lfloor v_2 \rfloor}+\vecdd{d_1}{d_2})\\
  =(1-d_1)(1-d_2)\vec{g}(\vec{h}_{\emptyset}(\vec{v}))+(1-d_2)d_1\vec{g}(\vec{h}_{\{1\}}(\vec{v}))+(1-d_1)d_2\vec{g}(\vec{h}_{\{2\}}(\vec{v}))
  +d_1d_2\vec{g}(\vec{h}_{\{1,2\}}(\vec{v}))
\end{align*}
\newpage
\section{partial velocity vector}
In order to calculate the momentum between two colliding masses it is necessary to determine the partial velocity vector of a moving mass in direction to the second mass.
Let's assume a mass $m_1$ moves with velocity $\vec{v}$ and would hit mass $m_2$. For simplicity $m_2$ remains stationary and the shapes of both masses are spherical.
Even when the direction of $\vec{v}$ does not point directly to $m_2$, it behaves as if $m_1$ hits $m_2$ with a (partial) velocity $\vec{v_{m_2}}$ which points to the
direction of $m_2$. The calculation holds for any dimension.

\begin{center}
\psset{xunit=1pt,yunit=1pt,runit=1pt}
\begin{pspicture}(0,150)(600,350)
  \newcommand{\msx}{150}
  \newcommand{\msy}{254}
  \newcommand{\bradius}{30}
  \newcommand{\cradius}{2}
  \newcommand{\mtx}{199}
  \newcommand{\mty}{220}
  \newcommand{\vlen}{273}
  \FPeval{\rsy}{clip(\msy+\bradius)}
  \FPeval{\rty}{clip(\mty+\bradius)}
  \FPeval{\lrsy}{clip(\msy+\bradius/2)}
  \FPeval{\lrty}{clip(\mty+\bradius/2)}
  \FPeval{\mvlsx}{clip(\msx-75)}
  \FPeval{\mvlsy}{clip(\msy)}
  \FPeval{\mvltx}{clip(\mtx+140)}
  \FPeval{\mvlty}{clip(\msy)}
  \FPeval{\mmlsx}{clip(\msx-75)}
  \FPeval{\mmlsy}{clip(\msy+51)}
  \FPeval{\mmltx}{clip(\mtx+75)}
  \FPeval{\mmlty}{clip(\mty-51)}
  \FPeval{\vx}{clip(\msx+(\vlen-\msx)/2)}
  \FPeval{\vmsy}{clip(\msy+3)}
  \FPeval{\vmty}{clip(\mty-25+3)}
  \FPeval{\vmtx}{clip(\mtx+35+3)}
  \FPeval{\mxra}{clip(\vmtx-2)}
  \FPeval{\myra}{clip(\vmty+7)}
  \FPeval{\dx}{clip(\msx+(\mtx-\msx)/2)}
  \FPeval{\dy}{clip(\mty+(\msy-\mty)/2)}
  \FPeval{\sx}{clip(\vmtx+(\vlen-\vmtx)/2)}
  \FPeval{\sy}{clip(\vmty+(\msy-\vmty)/2)}

  % m1
  \pscircle(\msx,\msy){\bradius}
  \pscircle*(\msx,\msy){\cradius}
  \uput[dl](\msx,\msy){$m_1$}
  \psline[]{-}(\msx,\msy)(\msx,\rsy)
  \uput[r](\msx,\lrsy){$r_1$}

  %m2
  \pscircle(\mtx,\mty){\bradius}
  \pscircle*(\mtx,\mty){\cradius}
  \uput[dl](\mtx,\mty){$m_2$}
  \psline[]{-}(\mtx,\mty)(\mtx,\rty)
  \uput[r](\mtx,\lrty){$r_2$}

  % movement line
  \psline[linestyle=dashed]{-}(\mvlsx,\mvlsy)(\mvltx,\mvlty)
  \uput[u](\mvltx,\mvlty){movement line}

  % vector v
  \psline[arrowscale=2]{->}(\msx,\msy)(\vlen,\msy)
  \uput[u](\vx,\msy){$\vec{v}$}

  % momentum line
  \psline[linestyle=dashed]{-}(\mmlsx,\mmlsy)(\mmltx,\mmlty)
  \uput[ur](\mmltx,\mmlty){momentum line}

  % vector d
  \psline[arrowscale=2,linecolor=blue]{->}(\msx,\msy)(\mtx,\mty)
  \uput[d](\dx,\dy){\textcolor{blue}{$\vec{d}$}}

  % vector vm2
  \psline[arrowscale=2,linecolor=red]{->}(\msx,\vmsy)(\vmtx,\vmty)
  \uput[dl](\vmtx,\vmty){\textcolor{red}{$\vec{v_{m_2}}$}}

  % vector s
  \psline[arrowscale=2,linecolor=olive]{->}(\vlen,\msy)(\vmtx,\vmty)
  \uput[dr](\sx,\sy){\textcolor{olive}{$\vec{s}$}}

  % right angle
  \psarc{-}(\vmtx,\vmty){15}{60}{140}
  \pscircle*(\mxra,\myra){1}

\end{pspicture}
\captionof{figure}{collision}
\label{fig:coll}
\end{center}

We need to calculate $\vec{v_{m_2}}$. We define $\vec{d}=\vec{m_2}-\vec{m_1}$, where $\vec{m_1}$ and $\vec{m_2}$ are the positional vectors for $m_1$ resp. $m_2$.
We require that the masses have a positive expansion ($r_1>0$, $r_2>0$), thus $\vec{d} \neq \vec{0}$ (collision takes place if
$|\vec{d}|=r_1+r_2$). Let $\vec{s}$ be a (the) vector with
$\vec{v_{m_2}}=\vec{v}+\vec{s}$. Then $\vec{s}$ must be orthogonal to $\vec{v_{m_2}}$ and thus to $\vec{d}$. That is because $\vec{v_{m_2}}$ is a partial vector of $\vec{v}$
and points in the same direction as $\vec{d}$ which resides on the momentum line. See figure \ref{fig:coll}.\\

\pagebreak
Be $\lambda \in \mathbb{R}$, then the following equations hold:

\begin{equation}
  \vec{v_{m_2}}=\vec{v}+\vec{s}=\lambda \vec{d}
\label{eq:vs}
\end{equation}
\begin{equation}
  \vec{s}\cdot\vec{d}=0
  \label{eq:sd}
\end{equation}

This can easily be solved:

\begin{align}
(\ref{eq:vs}),(\ref{eq:sd})=> (\lambda \vec{d}-\vec{v})\vec{d}=0\\
<=> \lambda\vec{d}^2-\vec{v}\vec{d}=0\\
<=> \lambda=\frac{\vec{v}\vec{d}}{\vec{d}^2}
\end{align}

As a result we get:

\begin{displaymath}
  \vec{v_{m_2}}=\frac{\vec{v}\vec{d}}{\vec{d}^2}\vec{d}
\end{displaymath}
\\
$\square$

\newpage
\section{physical line adjustment}
Points $P1,P2$ with $\vec{d}:=\vec{P2}-\vec{P1}, l:=$ required length of line\\\\
We obtain $P1',P2'$ with $|\vec{P2'}-\vec{P1'}|=l$ by calculating $\vec{P2'}=\vec{P2}-\vec{\Delta d}$ and $\vec{P1'}=\vec{P1}+\vec{\Delta d}$ with
$|\vec{\Delta d}|=\frac{|\vec{d}|-l}{2}$:\\\\
$\vec{\Delta d}=\frac{\vec{d}}{|\vec{d}|}*|\vec{\Delta d}|=\frac{|\vec{d}|-l}{2|\vec{d}|}\vec{d}$.

\newpage
\section{n-dimensional polar coordinates}
\subsection*{cartesian to polar}
$(x_1,\dots,x_n) \rightarrow (r,\alpha_1,\dots,\alpha_{n-1})$\\
\\
$\alpha_1=\arctan_2(\frac{x_2}{x_1})$\\\\
$\alpha_i=\arctan_2(\frac{x_{i+1}}{\sqrt{\sum_{j=1}^i{x_n^2}}})$
\subsection*{polar to cartesion}
$(r,\alpha_1,\dots,\alpha_{n-1}) \rightarrow (x_1,\dots,x_n)$\\
\\
$x_1=r\prod_{j=1}^{n_{\alpha}}{\cos\alpha_j}$\\
$x_i=r\sin\alpha_{i-1}\prod_{j=i}^{n_{\alpha}}\cos\alpha_j \spc\spc \forall\spc 1<i<=n$
\end{document}
